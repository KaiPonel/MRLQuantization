{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2293,"status":"ok","timestamp":1723654858621,"user":{"displayName":"Kai Ponel","userId":"09877000230544749752"},"user_tz":-120},"id":"DovlhcyLrdNW"},"outputs":[],"source":["import os\n","import json\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18055,"status":"ok","timestamp":1723654876674,"user":{"displayName":"Kai Ponel","userId":"09877000230544749752"},"user_tz":-120},"id":"juRYqL7ud71g","outputId":"5deb92bb-e25c-49f5-95e1-ab95e4980b10"},"outputs":[],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1723654876674,"user":{"displayName":"Kai Ponel","userId":"09877000230544749752"},"user_tz":-120},"id":"PB5Qywy_d8GP"},"outputs":[],"source":["BASE_PATH = \"/content/drive/MyDrive/BachelorThesisResults\"\n","DEFAULT_OFFSET_PATH = \"no_model_name_available/no_revision_available\"\n","GLOBAL_SAVE_DIR = \"/content/drive/MyDrive/BachelorThesisResults/plots_new\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1723654876674,"user":{"displayName":"Kai Ponel","userId":"09877000230544749752"},"user_tz":-120},"id":"CeOdjNdX95BG"},"outputs":[],"source":["tasks_dict = {\n","    \"classification\": [\n","        \"Banking77Classification\",\n","        \"EmotionClassification\",\n","        \"TweetSentimentExtractionClassification\",\n","        \"AmazonCounterfactualClassification\",\n","        \"MassiveIntentClassification\",\n","        \"MassiveScenarioClassification\",\n","        \"MTOPDomainClassification\",\n","        \"MTOPIntentClassification\"\n","    ],\n","    \"clustering\": [\n","        \"ArXivHierarchicalClusteringP2P\",\n","        \"ArXivHierarchicalClusteringS2S\",\n","        \"BiorxivClusteringP2P.v2\",\n","        \"BiorxivClusteringS2S.v2\",\n","        \"MedrxivClusteringP2P.v2\",\n","        \"MedrxivClusteringS2S.v2\",\n","        \"RedditClustering.v2\",\n","        \"StackExchangeClustering.v2\",\n","        \"StackExchangeClusteringP2P.v2\",\n","        \"TwentyNewsgroupsClustering.v2\",\n","    ],\n","    \"sts\": [\n","        \"BIOSSES\",\n","        \"SICK-R\",\n","        \"STS12\",\n","        \"STS13\",\n","        \"STS14\",\n","        \"STS15\",\n","        \"STS16\",\n","        \"STSBenchmark\",\n","        \"STS17\",\n","        \"STS22\",\n","    ],\n","    \"pairclass\": [\n","        \"SprintDuplicateQuestions\",\n","        \"TwitterSemEval2015\",\n","        \"TwitterURLCorpus\",\n","    ],\n","    \"retrieval\": [\n","        \"ArguAna\",\n","        \"CQADupstackWebmastersRetrieval\",\n","        \"NFCorpus\",\n","    ],\n","    \"rerank\": [\n","        \"AskUbuntuDupQuestions\",\n","        \"MindSmallReranking\",\n","        \"StackOverflowDupQuestions\"\n","    ],\n","    \"summ\": [\n","        \"SummEval\"\n","    ],\n","    \"all\": [\n","        \"Banking77Classification\",\n","        \"EmotionClassification\",\n","        \"TweetSentimentExtractionClassification\",\n","        \"AmazonCounterfactualClassification\",\n","        \"MassiveIntentClassification\",\n","        \"MassiveScenarioClassification\",\n","        \"MTOPDomainClassification\",\n","        \"MTOPIntentClassification\",\n","        \"ArXivHierarchicalClusteringP2P\",\n","        \"ArXivHierarchicalClusteringS2S\",\n","        \"BiorxivClusteringP2P.v2\",\n","        \"BiorxivClusteringS2S.v2\",\n","        \"MedrxivClusteringP2P.v2\",\n","        \"MedrxivClusteringS2S.v2\",\n","        \"RedditClustering.v2\",\n","        \"StackExchangeClustering.v2\",\n","        \"StackExchangeClusteringP2P.v2\",\n","        \"TwentyNewsgroupsClustering.v2\",\n","        \"BIOSSES\",\n","        \"SICK-R\",\n","        \"STS12\",\n","        \"STS13\",\n","        \"STS14\",\n","        \"STS15\",\n","        \"STS16\",\n","        \"STSBenchmark\",\n","        \"STS17\",\n","        \"STS22\",\n","        \"SprintDuplicateQuestions\",\n","        \"TwitterSemEval2015\",\n","        \"TwitterURLCorpus\",\n","        \"ArguAna\",\n","        \"CQADupstackWebmastersRetrieval\",\n","        \"NFCorpus\",\n","        \"AskUbuntuDupQuestions\",\n","        \"MindSmallReranking\",\n","        \"StackOverflowDupQuestions\",\n","        \"SummEval\"\n","    ]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1723654876674,"user":{"displayName":"Kai Ponel","userId":"09877000230544749752"},"user_tz":-120},"id":"USk8FlEmJL8i"},"outputs":[],"source":["models_dict = {\n","    \"nomic-embed-text-v1.5\": {\n","        \"path\": f\"{BASE_PATH}/v1/nomic-ai\",\n","        \"dims\": [768, 512, 256, 128, 64, 32, 16, 8],\n","        \"quantization_techniques\": [\"float32\", \"int8\", \"binary\"],\n","        \"offset_path\": DEFAULT_OFFSET_PATH\n","    },\n","    \"mxbai-embed-large-v1\": {\n","        \"path\": f\"{BASE_PATH}/v1/mixedbread-ai\",\n","        \"dims\": [1024, 512, 256, 128, 64, 32, 16, 8],\n","        \"quantization_techniques\": [\"float32\", \"int8\", \"binary\"],\n","        \"offset_path\": DEFAULT_OFFSET_PATH\n","    },\n","    \"stella_en_400M_v5\": {\n","        \"path\": f\"{BASE_PATH}/v1/dunzhang\",\n","        \"dims\": [8192, 4096, 2048, 1024, 512, 256, 128, 64],\n","        \"quantization_techniques\": [\"float32\", \"int8\", \"binary\"],\n","        \"offset_path\": DEFAULT_OFFSET_PATH\n","    }\n","}\n"]},{"cell_type":"markdown","metadata":{"id":"SaW6E-IyIY1K"},"source":["# Performance"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1723654876674,"user":{"displayName":"Kai Ponel","userId":"09877000230544749752"},"user_tz":-120},"id":"EFHN3N0890CD"},"outputs":[],"source":["def get_main_score(base_path, model_name, embedding_size, quantization_method, benchmark_name, offset_path=\"no_model_name_available/no_revision_available\"):\n","    \"\"\"\n","    Function 1: Return the main_score for one model, one embedding size, one quantization type, and one benchmark.\n","\n","    Args:\n","    base_path (str): Base directory containing the model folders.\n","    model_name (str): Name of the model.\n","    embedding_size (int): Embedding size of the model.\n","    quantization_method (str): Quantization method used.\n","    benchmark_name (str): Name of the benchmark.\n","\n","    Returns:\n","    float: The main_score for the specified configuration.\n","    \"\"\"\n","\n","    subfolder = f\"{model_name}_{embedding_size}_{quantization_method}\"\n","    file_path = os.path.join(base_path, subfolder, offset_path, f\"{benchmark_name}.json\")\n","\n","    try:\n","        with open(file_path, 'r') as file:\n","            data = json.load(file)\n","            main_score = data[\"scores\"][\"test\"][0][\"main_score\"]\n","            return main_score\n","    except (FileNotFoundError, KeyError) as e:\n","        print(f\"Error reading file {file_path}: {e}\")\n","        return None\n","\n","def load_scores(base_path, model_name, dims, quantization_techniques, tasks, offset_path):\n","    scores = {task: {technique: [] for technique in quantization_techniques} for task in tasks}\n","    for task in tasks:\n","        for technique in quantization_techniques:\n","            for dim in dims:\n","                score = get_main_score(base_path, model_name, dim, technique, task, offset_path=offset_path)\n","                if score is not None:\n","                    scores[task][technique].append(score)\n","    return scores\n","\n","def is_valid_task_category(category):\n","    return category in [\"classification\", \"clustering\", \"sts\", \"pairclass\", \"retrieval\", \"rerank\", \"summ\", \"all\"]\n","\n","def plot_task_scores(scores, dims, global_save_dir, model_name, task_category, task, plot_figures):\n","    if not is_valid_task_category(task_category):\n","        raise ValueError(\"Invalid task category. Received {}, but expected one of: classification, clustering, sts, pairclass, retrieval, rerank, summ\".format(task_category))\n","    for technique, technique_scores in scores[task].items():\n","        if len(technique_scores) == len(dims):\n","            plt.plot(dims, technique_scores, label=technique)\n","        else:\n","            print(f\"Skipping plot for {technique} on task {task} due to dimension mismatch\")\n","\n","    plt.xscale('log', base=2)\n","    plt.xticks(dims, [str(dim) for dim in dims])\n","    plt.xlabel('Dimension (base 10)')\n","    plt.ylabel('Score')\n","    plt.ylim(-0.15, 1.05)\n","    plt.legend()\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}/{task}.png\")\n","    if plot_figures:\n","      plt.show()\n","    plt.clf()\n","\n","def plot_technique_summary(scores, dims, global_save_dir, model_name, quantization_techniques, task_category, plot_figures):\n","    if not is_valid_task_category(task_category):\n","        raise ValueError(\"Invalid task category. Received {}, but expected one of: classification, clustering, sts, pairclass, retrieval, rerank, summ\".format(task_category))\n","\n","    for technique in quantization_techniques:\n","        all_scores = [task_scores[technique] for task_scores in scores.values() if len(task_scores[technique]) == len(dims)]\n","\n","        if not all_scores:\n","            continue\n","\n","        average_scores = np.mean(all_scores, axis=0)\n","\n","        plt.figure(figsize=(10, 5))\n","\n","        for task, task_scores in zip(scores.keys(), all_scores):\n","            sns.lineplot(x=dims, y=task_scores, color='gray', alpha=0.7)\n","\n","        plt.plot(dims, average_scores, color='red', linestyle='--', linewidth=3, label='Average')\n","\n","        plt.xscale('log', base=2)\n","        plt.xticks(dims, [str(dim) for dim in dims])\n","        plt.xlabel('Dimension (base 10)')\n","        plt.ylabel('Score')\n","        plt.ylim(-0.15, 1.05)\n","\n","        plt.grid(True, which=\"both\", ls=\"--\")\n","\n","        plt.legend(loc='best')\n","\n","        if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\"):\n","            os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\")\n","\n","        plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}/technique_{technique}.png\")\n","        if plot_figures:\n","            plt.show()\n","        plt.clf()\n","\n","def plot_all_technique_averages(scores, dims, global_save_dir, model_name, quantization_techniques, task_category, plot_figures):\n","    plt.figure(figsize=(10, 5))\n","\n","    for technique in quantization_techniques:\n","        all_scores = [task_scores[technique] for task_scores in scores.values() if len(task_scores[technique]) == len(dims)]\n","\n","        if not all_scores:\n","            continue\n","\n","        average_scores = np.mean(all_scores, axis=0)\n","        plt.plot(dims, average_scores, linestyle='--', linewidth=2, label=technique)\n","\n","    plt.xscale('log', base=2)\n","    plt.xticks(dims, [str(dim) for dim in dims])\n","    plt.xlabel('Dimension (base 10)')\n","    plt.ylabel('Score')\n","    plt.ylim(-0.15, 1.05)\n","\n","\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    plt.legend(loc='best')\n","\n","    if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}/all_averages.png\")\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","\n","def plot_all_technique_averages_relative(scores, dims, global_save_dir, model_name, quantization_techniques, task_category, plot_figures):\n","    plt.figure(figsize=(10, 5))\n","\n","\n","    reference_scores = {task: None for task in scores.keys()}\n","\n","    for task, task_scores in scores.items():\n","        if 'float32' in task_scores and len(task_scores['float32']) > 0:\n","            reference_scores[task] = task_scores['float32'][0]\n","        else:\n","            print(f\"Skipping task {task} as it doesn't have float32 scores.\")\n","            continue\n","\n","    for technique in quantization_techniques:\n","        all_relative_scores = []\n","        for task, task_scores in scores.items():\n","            if technique not in task_scores or len(task_scores[technique]) != len(dims):\n","                print(f\"Skipping task {task} for technique {technique} as it has missing results.\")\n","                continue\n","\n","            relative_scores = []\n","            for idx, score in enumerate(task_scores[technique]):\n","                if reference_scores[task] is not None and reference_scores[task] != 0:\n","                    relative_scores.append(score / reference_scores[task])\n","                else:\n","                    relative_scores.append(score)  \n","\n","            all_relative_scores.append(relative_scores)\n","\n","        if not all_relative_scores:\n","            continue\n","\n","        average_relative_scores = np.mean(all_relative_scores, axis=0)\n","        plt.plot(dims, average_relative_scores, linestyle='--', linewidth=2, label=technique)\n","\n","    plt.xscale('log', base=2)\n","    plt.xticks(dims, [str(dim) for dim in dims])\n","    plt.xlabel('Dimension (base 10)')\n","    plt.ylabel('Relative Score')\n","    plt.ylim(-0.15, 1.05)\n","\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    plt.legend(loc='best')\n","\n","    save_path = f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\"\n","    if not os.path.exists(save_path):\n","        os.makedirs(save_path)\n","\n","    plt.savefig(f\"{save_path}/all_averages_relative.png\")\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","def generate_results(base_path, model_name, dims, quantization_techniques, tasks_classification, task_category, model_offset_path, plot_figures=True):\n","    scores = load_scores(base_path, model_name, dims, quantization_techniques, tasks_classification, MODEL_OFFSET_PATH)\n","\n","    if task_category != \"all\":\n","      for task in tasks_classification:\n","          plot_task_scores(scores, dims, GLOBAL_SAVE_DIR, MODEL_NAME, task_category, task, plot_figures)\n","\n","    plot_technique_summary(scores, dims, GLOBAL_SAVE_DIR, MODEL_NAME, quantization_techniques, task_category, plot_figures)\n","    plot_all_technique_averages(scores, dims, GLOBAL_SAVE_DIR, MODEL_NAME, quantization_techniques, task_category, plot_figures)\n","    plot_all_technique_averages_relative(scores, dims, GLOBAL_SAVE_DIR, MODEL_NAME, quantization_techniques, task_category, plot_figures)"]},{"cell_type":"markdown","metadata":{"id":"-eYDvh5wFAFX"},"source":["## Stella en_400M_v5"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1723654876674,"user":{"displayName":"Kai Ponel","userId":"09877000230544749752"},"user_tz":-120},"id":"IpXqQBw5d8Id"},"outputs":[],"source":["MODEL_NAME = \"stella_en_400M_v5\"\n","MODEL_OFFSET_PATH = models_dict[MODEL_NAME][\"offset_path\"]\n","MODEL_BASE_PATH = models_dict[MODEL_NAME][\"path\"]\n","dims = models_dict[MODEL_NAME][\"dims\"]\n","quantization_techniques = models_dict[MODEL_NAME][\"quantization_techniques\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"gRafq_k2Z435"},"outputs":[],"source":["for task_category, tasks in tasks_dict.items():\n","    generate_results(MODEL_BASE_PATH, MODEL_NAME, dims, quantization_techniques, tasks, task_category, MODEL_OFFSET_PATH, plot_figures=False)"]},{"cell_type":"markdown","metadata":{"id":"8YAzlV7EAwgj"},"source":["## MixedBread-AI V1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gL1jGAl_AyHc"},"outputs":[],"source":["MODEL_NAME = \"mxbai-embed-large-v1\"\n","MODEL_OFFSET_PATH = models_dict[MODEL_NAME][\"offset_path\"]\n","MODEL_BASE_PATH = models_dict[MODEL_NAME][\"path\"]\n","dims = models_dict[MODEL_NAME][\"dims\"]\n","quantization_techniques = models_dict[MODEL_NAME][\"quantization_techniques\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"34jCQNKZaY1f"},"outputs":[],"source":["for task_category, tasks in tasks_dict.items():\n","    generate_results(MODEL_BASE_PATH, MODEL_NAME, dims, quantization_techniques, tasks, task_category, MODEL_OFFSET_PATH, plot_figures=False)"]},{"cell_type":"markdown","metadata":{"id":"R6Pw6xLDA7VF"},"source":["## Nomic-text-embed-v1.5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"V2h4W9udA-Qn"},"outputs":[],"source":["MODEL_NAME = \"nomic-embed-text-v1.5\" \n","MODEL_BASE_PATH = models_dict[MODEL_NAME][\"path\"]\n","MODEL_OFFSET_PATH = models_dict[MODEL_NAME][\"offset_path\"]\n","dims = models_dict[MODEL_NAME][\"dims\"]\n","quantization_techniques = models_dict[MODEL_NAME][\"quantization_techniques\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AXZSM0pHaZwl"},"outputs":[],"source":["for task_category, tasks in tasks_dict.items():\n","    generate_results(MODEL_BASE_PATH, MODEL_NAME, dims, quantization_techniques, tasks, task_category, MODEL_OFFSET_PATH, plot_figures=False)"]},{"cell_type":"markdown","metadata":{"id":"EUexSX71TFuL"},"source":["# Time"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aFC140NATJYe"},"outputs":[],"source":["def get_evaluation_time(base_path, model_name, embedding_size, quantization_method, benchmark_name, offset_path=\"no_model_name_available/no_revision_available\"):\n","    \"\"\"\n","    Function: Return the evaluation_time for one model, one embedding size, one quantization type, and one benchmark.\n","\n","    Args:\n","    base_path (str): Base directory containing the model folders.\n","    model_name (str): Name of the model.\n","    embedding_size (int): Embedding size of the model.\n","    quantization_method (str): Quantization method used.\n","    benchmark_name (str): Name of the benchmark.\n","\n","    Returns:\n","    float: The evaluation_time for the specified configuration.\n","    \"\"\"\n","\n","    subfolder = f\"{model_name}_{embedding_size}_{quantization_method}\"\n","    file_path = os.path.join(base_path, subfolder, offset_path, f\"{benchmark_name}.json\")\n","\n","    try:\n","        with open(file_path, 'r') as file:\n","            data = json.load(file)\n","            evaluation_time = data[\"evaluation_time\"]\n","            return evaluation_time\n","    except (FileNotFoundError, KeyError) as e:\n","        print(f\"Error reading file {file_path}: {e}\")\n","        return None\n","\n","def load_evaluation_times(base_path, model_name, dims, quantization_techniques, tasks, offset_path):\n","    times = {task: {technique: [] for technique in quantization_techniques} for task in tasks}\n","    for task in tasks:\n","        for technique in quantization_techniques:\n","            for dim in dims:\n","                time = get_evaluation_time(base_path, model_name, dim, technique, task, offset_path=offset_path)\n","                if time is not None:\n","                    times[task][technique].append(time)\n","    return times\n","\n","def plot_task_evaluation_times(times, dims, global_save_dir, model_name, task_category, task, plot_figures):\n","    if not is_valid_task_category(task_category):\n","        raise ValueError(\"Invalid task category. Received {}, but expected one of: classification, clustering, sts, pairclass, retrieval, rerank, summ\".format(task_category))\n","    for technique, technique_times in times[task].items():\n","        if len(technique_times) == len(dims):\n","            plt.plot(dims, technique_times, label=technique)\n","        else:\n","            print(f\"Skipping plot for {technique} on task {task} due to dimension mismatch\")\n","\n","    plt.xscale('log', base=2)\n","    plt.xticks(dims, [str(dim) for dim in dims])\n","    plt.xlabel('Dimension (base 10)')\n","    plt.ylabel('Evaluation Time (s)')\n","    plt.legend()\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","\n","    if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}\")\n","\n","\n","    plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}/{task}.png\")\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","def plot_technique_evaluation_time_summary(times, dims, global_save_dir, model_name, quantization_techniques, task_category, plot_figures):\n","    if not is_valid_task_category(task_category):\n","        raise ValueError(\"Invalid task category. Received {}, but expected one of: classification, clustering, sts, pairclass, retrieval, rerank, summ\".format(task_category))\n","\n","    for technique in quantization_techniques:\n","        all_times = [task_times[technique] for task_times in times.values() if len(task_times[technique]) == len(dims)]\n","\n","        if not all_times:\n","            continue\n","\n","        average_times = np.mean(all_times, axis=0)\n","\n","        plt.figure(figsize=(10, 5))\n","\n","        for task, task_times in zip(times.keys(), all_times):\n","            sns.lineplot(x=dims, y=task_times, color='gray', alpha=0.7)\n","\n","        plt.plot(dims, average_times, color='red', linestyle='--', linewidth=3, label='Average')\n","\n","        plt.xscale('log', base=2)\n","        plt.xticks(dims, [str(dim) for dim in dims])\n","        plt.xlabel('Dimension (base 10)')\n","        plt.ylabel('Evaluation Time (s)')\n","        plt.grid(True, which=\"both\", ls=\"--\")\n","\n","        plt.legend(loc='best')\n","\n","        if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}\"):\n","            os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}\")\n","\n","        plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}/technique_{technique}.png\")\n","        if plot_figures:\n","            plt.show()\n","        plt.clf()\n","\n","def plot_all_technique_evaluation_time_averages(times, dims, global_save_dir, model_name, quantization_techniques, task_category, plot_figures):\n","    plt.figure(figsize=(10, 5))\n","\n","    for technique in quantization_techniques:\n","        all_times = [task_times[technique] for task_times in times.values() if len(task_times[technique]) == len(dims)]\n","\n","        if not all_times:\n","            continue\n","\n","        average_times = np.mean(all_times, axis=0)\n","        plt.plot(dims, average_times, linestyle='--', linewidth=2, label=technique)\n","\n","    plt.xscale('log', base=2)\n","    plt.xticks(dims, [str(dim) for dim in dims])\n","    plt.xlabel('Dimension (base 10)')\n","    plt.ylabel('Evaluation Time (s)')\n","\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    plt.legend(loc='best')\n","\n","    if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}/all_averages.png\")\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","def plot_all_technique_evaluation_time_averages_relative(times, dims, global_save_dir, model_name, quantization_techniques, task_category, plot_figures):\n","    plt.figure(figsize=(10, 5))\n","\n","    for technique in quantization_techniques:\n","        relative_times = []\n","        for task, task_times in times.items():\n","            if len(task_times['float32']) == len(dims) and len(task_times[technique]) == len(dims):\n","                reference_time = task_times['float32'][0]  \n","                relative_task_times = [t / reference_time for t in task_times[technique]]\n","                relative_times.append(relative_task_times)\n","\n","        if not relative_times:\n","            continue\n","\n","        average_relative_times = np.mean(relative_times, axis=0)\n","        plt.plot(dims, average_relative_times, linestyle='--', linewidth=2, label=technique)\n","\n","    plt.xscale('log', base=2)\n","    plt.xticks(dims, [str(dim) for dim in dims])\n","    plt.xlabel('Dimension (base 10)')\n","    plt.ylabel('Relative Evaluation Time')\n","\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    plt.legend(loc='best')\n","\n","    plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}/all_averages_evaluation_time_relative.png\")\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","def generate_evaluation_time_results(base_path, model_name, dims, quantization_techniques, tasks, task_category, model_offset_path, plot_figures=True):\n","    times = load_evaluation_times(base_path, model_name, dims, quantization_techniques, tasks, MODEL_OFFSET_PATH)\n","\n","    if task_category != \"all\":\n","      for task in tasks:\n","          plot_task_evaluation_times(times, dims, GLOBAL_SAVE_DIR, model_name, task_category, task, plot_figures)\n","\n","    plot_technique_evaluation_time_summary(times, dims, GLOBAL_SAVE_DIR, model_name, quantization_techniques, task_category, plot_figures)\n","    plot_all_technique_evaluation_time_averages(times, dims, GLOBAL_SAVE_DIR, model_name, quantization_techniques, task_category, plot_figures)\n","    plot_all_technique_evaluation_time_averages_relative(times, dims, GLOBAL_SAVE_DIR, model_name, quantization_techniques, task_category, plot_figures)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DXu5uuRobw7R"},"source":["## Stella en_400M_v5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9mMl4OShbw7W"},"outputs":[],"source":["MODEL_NAME = \"stella_en_400M_v5\"\n","MODEL_OFFSET_PATH = models_dict[MODEL_NAME][\"offset_path\"]\n","MODEL_BASE_PATH = models_dict[MODEL_NAME][\"path\"]\n","dims = models_dict[MODEL_NAME][\"dims\"]\n","quantization_techniques = models_dict[MODEL_NAME][\"quantization_techniques\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1C1ZRQvJ_I6-aCQR2e82uLEklVV79YnUo"},"id":"GgACVnKtbw7W","outputId":"c5ed6ee4-1444-429d-a647-2750df4f1106"},"outputs":[],"source":["for task_category, tasks in tasks_dict.items():\n","    generate_evaluation_time_results(MODEL_BASE_PATH, MODEL_NAME, dims, quantization_techniques, tasks, task_category, MODEL_OFFSET_PATH, plot_figures=True)"]},{"cell_type":"markdown","metadata":{"id":"1BmkFZ2Jbw7X"},"source":["## MixedBread-AI V1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vDS6Sn1ibw7X"},"outputs":[],"source":["MODEL_NAME = \"mxbai-embed-large-v1\"\n","MODEL_OFFSET_PATH = models_dict[MODEL_NAME][\"offset_path\"]\n","MODEL_BASE_PATH = models_dict[MODEL_NAME][\"path\"]\n","dims = models_dict[MODEL_NAME][\"dims\"]\n","quantization_techniques = models_dict[MODEL_NAME][\"quantization_techniques\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1n7sqgDwDkyNwiTgTkKKAmszsE4_ZIFW1"},"id":"v3kQ-wGObw7X","outputId":"e39c45a5-2af4-4def-9328-9a7dbf3bd1f0"},"outputs":[],"source":["for task_category, tasks in tasks_dict.items():\n","    generate_evaluation_time_results(MODEL_BASE_PATH, MODEL_NAME, dims, quantization_techniques, tasks, task_category, MODEL_OFFSET_PATH, plot_figures=True)"]},{"cell_type":"markdown","metadata":{"id":"0DiRPmvCbw7X"},"source":["## Nomic-text-embed-v1.5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aEPPPj5ubw7X"},"outputs":[],"source":["MODEL_NAME = \"nomic-embed-text-v1.5\"\n","MODEL_BASE_PATH = models_dict[MODEL_NAME][\"path\"]\n","MODEL_OFFSET_PATH = models_dict[MODEL_NAME][\"offset_path\"]\n","dims = models_dict[MODEL_NAME][\"dims\"]\n","quantization_techniques = models_dict[MODEL_NAME][\"quantization_techniques\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ni4AFJXVbw7X"},"outputs":[],"source":["for task_category, tasks in tasks_dict.items():\n","    generate_evaluation_time_results(MODEL_BASE_PATH, MODEL_NAME, dims, quantization_techniques, tasks, task_category, MODEL_OFFSET_PATH, plot_figures=False)"]},{"cell_type":"markdown","metadata":{"id":"uehX66hGndS2"},"source":["# Accuracy Compute TradeOff"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5M8KDdkMnnGa"},"outputs":[],"source":["def get_main_score(base_path, model_name, embedding_size, quantization_method, benchmark_name, offset_path=\"no_model_name_available/no_revision_available\"):\n","    \"\"\"\n","    Function 1: Return the main_score for one model, one embedding size, one quantization type, and one benchmark.\n","\n","    Args:\n","    base_path (str): Base directory containing the model folders.\n","    model_name (str): Name of the model.\n","    embedding_size (int): Embedding size of the model.\n","    quantization_method (str): Quantization method used.\n","    benchmark_name (str): Name of the benchmark.\n","\n","    Returns:\n","    float: The main_score for the specified configuration.\n","    \"\"\"\n","\n","    subfolder = f\"{model_name}_{embedding_size}_{quantization_method}\"\n","    file_path = os.path.join(base_path, subfolder, offset_path, f\"{benchmark_name}.json\")\n","\n","    try:\n","        with open(file_path, 'r') as file:\n","            data = json.load(file)\n","            main_score = data[\"scores\"][\"test\"][0][\"main_score\"]\n","            return main_score\n","    except (FileNotFoundError, KeyError) as e:\n","        print(f\"Error reading file {file_path}: {e}\")\n","        return None\n","\n","def load_scores(base_path, model_name, dims, quantization_techniques, tasks, offset_path):\n","    scores = {task: {technique: [] for technique in quantization_techniques} for task in tasks}\n","    for task in tasks:\n","        for technique in quantization_techniques:\n","            for dim in dims:\n","                score = get_main_score(base_path, model_name, dim, technique, task, offset_path=offset_path)\n","                if score is not None:\n","                    scores[task][technique].append(score)\n","    return scores\n","\n","def calculate_memory_used(embedding_size, quantization_method):\n","    if quantization_method == 'float32':\n","        return embedding_size * 32\n","    elif quantization_method == 'int8':\n","        return embedding_size * 8\n","    elif quantization_method == 'binary':\n","        return embedding_size * 1\n","    else:\n","        raise ValueError(f\"Unknown quantization method: {quantization_method}\")\n","\n","def plot_accuracy_compute_tradeoff(scores, dims, quantization_techniques, global_save_dir, model_name, task_category, task, plot_figures):\n","    plt.figure(figsize=(10, 5))\n","    colors = sns.color_palette(\"husl\", len(quantization_techniques))\n","    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', '+'] \n","\n","    for technique_idx, technique in enumerate(quantization_techniques):\n","        technique_scores = scores[task][technique]\n","        if len(technique_scores) == len(dims):\n","            memory_used = [calculate_memory_used(dim, technique) for dim in dims]\n","            for idx, (mem, score) in enumerate(zip(memory_used, technique_scores)):\n","                plt.scatter(mem, score, label=f\"{technique}\" if idx == 0 else \"\", \n","                            color=colors[technique_idx], marker=markers[idx % len(markers)], s=100, edgecolor='k')\n","\n","    plt.xlabel('Memory Used (bits)')\n","    plt.ylabel('Performance Score')\n","    plt.xscale('log', base=2)\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    handles, labels = plt.gca().get_legend_handles_labels()\n","    by_label = dict(zip(labels, handles))\n","    legend1 = plt.legend(by_label.values(), by_label.keys(), loc='best')\n","    plt.gca().add_artist(legend1)\n","\n","    marker_handles = [plt.Line2D([0], [0], marker=marker, color='w', markerfacecolor='k', markersize=10, linestyle='None') for marker in markers[:len(dims)]]\n","    marker_labels = [f'{dim}d' for dim in dims]\n","    legend2 = plt.legend(marker_handles, marker_labels, loc='lower right', bbox_to_anchor=(1, 0))\n","\n","    if not os.path.exists(f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}/{task}.png\", bbox_inches='tight')\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","\n","def plot_average_accuracy_compute_tradeoff(scores, dims, quantization_techniques, global_save_dir, model_name, task_category, plot_figures):\n","    plt.figure(figsize=(12, 6))\n","    colors = sns.color_palette(\"husl\", len(quantization_techniques))\n","    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', '+'] \n","\n","    average_scores = {technique: [] for technique in quantization_techniques}\n","    for technique in quantization_techniques:\n","        all_scores = []\n","        for task, task_scores in scores.items():\n","            if len(task_scores[technique]) == len(dims):\n","                all_scores.append(task_scores[technique])\n","\n","        if all_scores:\n","            average_scores[technique] = np.mean(all_scores, axis=0)\n","\n","    for technique_idx, (technique, scores) in enumerate(average_scores.items()):\n","        if len(scores) > 0:\n","            memory_used = [calculate_memory_used(dim, technique) for dim in dims]\n","            for idx, (mem, score) in enumerate(zip(memory_used, scores)):\n","                plt.scatter(mem, score, label=f\"{technique}\" if idx == 0 else \"\",  \n","                            color=colors[technique_idx], marker=markers[idx % len(markers)], s=100, edgecolor='k')\n","\n","    plt.xlabel('Memory Used (bits)')\n","    plt.ylabel('Average Performance Score')\n","    plt.xscale('log', base=2)\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    handles, labels = plt.gca().get_legend_handles_labels()\n","    by_label = dict(zip(labels, handles))\n","    legend1 = plt.legend(by_label.values(), by_label.keys(), loc='best')\n","    plt.gca().add_artist(legend1)\n","\n","    marker_handles = [plt.Line2D([0], [0], marker=marker, color='w', markerfacecolor='k', markersize=10, linestyle='None') for marker in markers[:len(dims)]]\n","    marker_labels = [f'{dim}d' for dim in dims]\n","    legend2 = plt.legend(marker_handles, marker_labels, loc='lower right', bbox_to_anchor=(1, 0))\n","\n","    if not os.path.exists(f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}/all_averages.png\", bbox_inches='tight')\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","\n","def plot_average_accuracy_compute_tradeoff_relative(scores, dims, quantization_techniques, global_save_dir, model_name, task_category, plot_figures):\n","    plt.figure(figsize=(12, 6))\n","    colors = sns.color_palette(\"husl\", len(quantization_techniques))\n","    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', '+']  \n","\n","    average_scores = {technique: [] for technique in quantization_techniques}\n","\n","    reference_scores = {task: None for task in scores.keys()}\n","\n","    for task, task_scores in scores.items():\n","        if 'float32' in task_scores and len(task_scores['float32']) > 0:\n","            reference_scores[task] = task_scores['float32'][0]\n","        else:\n","            print(f\"Skipping task {task} as it doesn't have float32 scores.\")\n","            continue\n","\n","    for technique in quantization_techniques:\n","        all_relative_scores = []\n","        for task, task_scores in scores.items():\n","            if technique not in task_scores or len(task_scores[technique]) != len(dims):\n","                print(f\"Skipping task {task} for technique {technique} as it has missing results.\")\n","                continue\n","\n","            relative_scores = []\n","            for idx, score in enumerate(task_scores[technique]):\n","                if reference_scores[task] is not None and reference_scores[task] != 0:\n","                    relative_scores.append(score / reference_scores[task])\n","                else:\n","                    relative_scores.append(score) \n","\n","            all_relative_scores.append(relative_scores)\n","\n","        if all_relative_scores:\n","            average_scores[technique] = np.mean(all_relative_scores, axis=0)\n","\n","    for technique_idx, (technique, scores) in enumerate(average_scores.items()):\n","        if len(scores) > 0:\n","            memory_used = [calculate_memory_used(dim, technique) for dim in dims]\n","            for idx, (mem, score) in enumerate(zip(memory_used, scores)):\n","                plt.scatter(mem, score, label=f\"{technique}\" if idx == 0 else \"\", \n","                            color=colors[technique_idx], marker=markers[idx % len(markers)], s=100, edgecolor='k')\n","\n","    plt.xlabel('Memory Used (bits)')\n","    plt.ylabel('Relative Average Performance Score')\n","    plt.xscale('log', base=2)\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    handles, labels = plt.gca().get_legend_handles_labels()\n","    by_label = dict(zip(labels, handles))\n","    legend1 = plt.legend(by_label.values(), by_label.keys(), loc='best')\n","    plt.gca().add_artist(legend1)\n","\n","    marker_handles = [plt.Line2D([0], [0], marker=marker, color='w', markerfacecolor='k', markersize=10, linestyle='None') for marker in markers[:len(dims)]]\n","    marker_labels = [f'{dim}d' for dim in dims]\n","    legend2 = plt.legend(marker_handles, marker_labels, loc='lower right', bbox_to_anchor=(1, 0))\n","\n","    save_path = f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}\"\n","    if not os.path.exists(save_path):\n","        os.makedirs(save_path)\n","\n","    plt.savefig(f\"{save_path}/all_averages_relative.png\", bbox_inches='tight')\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sHMWwSUbnoa8"},"outputs":[],"source":["def generate_accuracy_compute_tradeoff_results(base_path, model_name, dims, quantization_techniques, tasks, task_category, model_offset_path, plot_figures=True):\n","    scores = load_scores(base_path, model_name, dims, quantization_techniques, tasks, model_offset_path)\n","\n","    if task_category != \"all\":\n","      for task in tasks:\n","          plot_accuracy_compute_tradeoff(scores, dims, quantization_techniques, GLOBAL_SAVE_DIR, model_name, task_category, task, plot_figures)\n","\n","    plot_average_accuracy_compute_tradeoff(scores, dims, quantization_techniques, GLOBAL_SAVE_DIR, model_name, task_category, plot_figures)\n","    plot_average_accuracy_compute_tradeoff_relative(scores, dims, quantization_techniques, GLOBAL_SAVE_DIR, model_name, task_category, plot_figures)\n"]},{"cell_type":"markdown","metadata":{"id":"XSSGGjk856Y6"},"source":["##  Stella en_400M_v5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bUjI32UE56Y_"},"outputs":[],"source":["MODEL_NAME = \"stella_en_400M_v5\"\n","MODEL_OFFSET_PATH = models_dict[MODEL_NAME][\"offset_path\"]\n","MODEL_BASE_PATH = models_dict[MODEL_NAME][\"path\"]\n","dims = models_dict[MODEL_NAME][\"dims\"]\n","quantization_techniques = models_dict[MODEL_NAME][\"quantization_techniques\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"14xkXe6Fdz_h0uHy3be1AX5Q80TdfpIC8"},"id":"rAuQC5hS56Y_","outputId":"6c4b841d-89b6-40b5-b282-0b781b97f273"},"outputs":[],"source":["for task_category, tasks in tasks_dict.items():\n","  generate_accuracy_compute_tradeoff_results(MODEL_BASE_PATH, MODEL_NAME, dims, quantization_techniques, tasks, task_category, MODEL_OFFSET_PATH, plot_figures=True)"]},{"cell_type":"markdown","metadata":{"id":"0ObuTrVX5uqo"},"source":["## Mixed-Bread-Ai v1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zwvuo2oyn5FZ"},"outputs":[],"source":["MODEL_NAME = \"mxbai-embed-large-v1\"\n","MODEL_OFFSET_PATH = models_dict[MODEL_NAME][\"offset_path\"]\n","MODEL_BASE_PATH = models_dict[MODEL_NAME][\"path\"]\n","dims = models_dict[MODEL_NAME][\"dims\"]\n","quantization_techniques = models_dict[MODEL_NAME][\"quantization_techniques\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1eg52zgFgzlym8DNbdWG6WuHFvOYu0TWH"},"id":"cdR3mRvtnq9s","outputId":"1146c6b5-6fef-4f7d-87bd-4e2b81ec3dd6"},"outputs":[],"source":["for task_category, tasks in tasks_dict.items():\n","  generate_accuracy_compute_tradeoff_results(MODEL_BASE_PATH, MODEL_NAME, dims, quantization_techniques, tasks, task_category, MODEL_OFFSET_PATH, plot_figures=True)"]},{"cell_type":"markdown","metadata":{"id":"4IbU5xfz50Ll"},"source":["## Nomic-text-embed-v1.5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"M2tEyQ5x50Lr"},"outputs":[],"source":["MODEL_NAME = \"nomic-embed-text-v1.5\"\n","MODEL_BASE_PATH = models_dict[MODEL_NAME][\"path\"]\n","MODEL_OFFSET_PATH = models_dict[MODEL_NAME][\"offset_path\"]\n","dims = models_dict[MODEL_NAME][\"dims\"]\n","quantization_techniques = models_dict[MODEL_NAME][\"quantization_techniques\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1KDIZG5dfDS_Y6IslwYBBykYR2NuRFXpz"},"id":"JiQ5vntd50Lr","outputId":"00e26ceb-0e1a-4808-837a-53e2f07bcb4e"},"outputs":[],"source":["for task_category, tasks in tasks_dict.items():\n","  generate_accuracy_compute_tradeoff_results(MODEL_BASE_PATH, MODEL_NAME, dims, quantization_techniques, tasks, task_category, MODEL_OFFSET_PATH, plot_figures=True)"]},{"cell_type":"markdown","metadata":{"id":"NPUDqu0DAPGa"},"source":["# 2D Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Y9juyADTBFjL"},"outputs":[],"source":["MODEL_NAME = \"mxbai-embed-2d-large-v1\"\n","MODEL_OFFSET_PATH = DEFAULT_OFFSET_PATH\n","MODEL_BASE_PATH = f\"{BASE_PATH}/2D/mixedbread-ai\"\n","dims = [1024, 512, 256, 128, 64, 32, 16, 8]\n","quantization_techniques = [\"float32\", \"int8\", \"binary\"]\n","INFERENCE_LAYERS = [24, 20, 16, 12]"]},{"cell_type":"markdown","metadata":{"id":"OuaGSO-hBFjM"},"source":["## Performance, Intramodel Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"I87jyiW6BFjL"},"outputs":[],"source":["\n","LINE_STYLES = {\n","    'float32': '-',\n","    'int8': '--',\n","    'binary': ':',\n","}\n","\n","\n","def get_main_score(base_path, model_name, embedding_size, quantization_method, benchmark_name, inference_layer, offset_path=\"no_model_name_available/no_revision_available\"):\n","    \"\"\"\n","    Function 1: Return the main_score for one model, one embedding size, one quantization type, one benchmark, and one inference layer.\n","\n","    Args:\n","    base_path (str): Base directory containing the model folders.\n","    model_name (str): Name of the model.\n","    embedding_size (int): Embedding size of the model.\n","    quantization_method (str): Quantization method used.\n","    benchmark_name (str): Name of the benchmark.\n","    inference_layer (int): Inference layer number.\n","\n","    Returns:\n","    float: The main_score for the specified configuration.\n","    \"\"\"\n","\n","    subfolder = f\"{model_name}_{embedding_size}_{quantization_method}_{inference_layer}\"\n","    file_path = os.path.join(base_path, subfolder, offset_path, f\"{benchmark_name}.json\")\n","\n","    try:\n","        with open(file_path, 'r') as file:\n","            data = json.load(file)\n","            main_score = data[\"scores\"][\"test\"][0][\"main_score\"]\n","            return main_score\n","    except (FileNotFoundError, KeyError) as e:\n","        print(f\"Error reading file {file_path}: {e}\")\n","        return None\n","\n","def load_scores(base_path, model_name, dims, quantization_techniques, tasks, inference_layers, offset_path):\n","    scores = {task: {technique: {layer: [] for layer in inference_layers} for technique in quantization_techniques} for task in tasks}\n","    for task in tasks:\n","        for technique in quantization_techniques:\n","            for dim in dims:\n","                for layer in inference_layers:\n","                    score = get_main_score(base_path, model_name, dim, technique, task, layer, offset_path=offset_path)\n","                    if score is not None:\n","                        scores[task][technique][layer].append(score)\n","    return scores\n","\n","def is_valid_task_category(category):\n","    return category in [\"classification\", \"clustering\", \"sts\", \"pairclass\", \"retrieval\", \"rerank\", \"summ\", \"all\"]\n","\n","def plot_task_scores(scores, dims, global_save_dir, model_name, task_category, task, plot_figures):\n","    if not is_valid_task_category(task_category):\n","        raise ValueError(\"Invalid task category. Received {}, but expected one of: classification, clustering, sts, pairclass, retrieval, rerank, summ\".format(task_category))\n","\n","    color_palette = sns.color_palette(\"husl\", len(scores[task][list(scores[task].keys())[0]].keys()))\n","    inference_layers = list(scores[task][list(scores[task].keys())[0]].keys())\n","    layer_color_map = {layer: color for layer, color in zip(inference_layers, color_palette)}\n","\n","    for technique, layers_scores in scores[task].items():\n","        for layer, technique_scores in layers_scores.items():\n","            if len(technique_scores) == len(dims):\n","                plt.plot(dims, technique_scores, linestyle=LINE_STYLES.get(technique, '-'), color=layer_color_map[layer], linewidth=2, label=f\"{technique} (Layer {layer})\")\n","            else:\n","                print(f\"Skipping plot for {technique} on task {task} at layer {layer} due to dimension mismatch\")\n","\n","    plt.xscale('log', base=2)\n","    plt.xticks(dims, [str(dim) for dim in dims])\n","    plt.xlabel('Dimension (base 10)')\n","    plt.ylabel('Score')\n","    plt.ylim(-0.15, 1.05)\n","    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), borderaxespad=0) \n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}/{task}.png\")\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","def plot_technique_summary(scores, dims, global_save_dir, model_name, quantization_techniques, inference_layers, task_category, plot_figures):\n","    if not is_valid_task_category(task_category):\n","        raise ValueError(\"Invalid task category. Received {}, but expected one of: classification, clustering, sts, pairclass, retrieval, rerank, summ\".format(task_category))\n","\n","    color_palette = sns.color_palette(\"husl\", len(inference_layers))\n","    layer_color_map = {layer: color for layer, color in zip(inference_layers, color_palette)}\n","\n","    for technique in quantization_techniques:\n","        plt.figure(figsize=(10, 5))\n","\n","        for layer in inference_layers:\n","            all_scores = [task_scores[technique][layer] for task_scores in scores.values() if len(task_scores[technique][layer]) == len(dims)]\n","\n","            if not all_scores:\n","                continue\n","\n","            average_scores = np.mean(all_scores, axis=0)\n","            plt.plot(dims, average_scores, linestyle=LINE_STYLES.get(technique, '-'), color=layer_color_map[layer], linewidth=2, label=f\"Layer {layer}\")\n","\n","        plt.xscale('log', base=2)\n","        plt.xticks(dims, [str(dim) for dim in dims])\n","        plt.xlabel('Dimension (base 10)')\n","        plt.ylabel('Score')\n","        plt.ylim(-0.15, 1.05)\n","\n","        plt.grid(True, which=\"both\", ls=\"--\")\n","\n","        plt.legend(loc='best')\n","\n","        if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\"):\n","            os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\")\n","\n","        plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}/technique_{technique}.png\")\n","        if plot_figures:\n","            plt.show()\n","        plt.clf()\n","\n","def plot_all_technique_averages(scores, dims, global_save_dir, model_name, quantization_techniques, inference_layers, task_category, plot_figures):\n","    plt.figure(figsize=(10, 5))\n","\n","    color_palette = sns.color_palette(\"husl\", len(inference_layers))\n","    layer_color_map = {layer: color for layer, color in zip(inference_layers, color_palette)}\n","\n","    for technique in quantization_techniques:\n","        for layer in inference_layers:\n","            all_scores = [task_scores[technique][layer] for task_scores in scores.values() if len(task_scores[technique][layer]) == len(dims)]\n","\n","            if not all_scores:\n","                continue\n","\n","            average_scores = np.mean(all_scores, axis=0)\n","            plt.plot(dims, average_scores, linestyle=LINE_STYLES.get(technique, '-'), color=layer_color_map[layer], linewidth=2, label=f\"{technique} (Layer {layer})\")\n","\n","    plt.xscale('log', base=2)\n","    plt.xticks(dims, [str(dim) for dim in dims])\n","    plt.xlabel('Dimension (base 10)')\n","    plt.ylabel('Score')\n","    plt.ylim(-0.15, 1.05)\n","\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), borderaxespad=0)\n","\n","    if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}/all_averages.png\", bbox_inches='tight')\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","def plot_all_technique_averages_relative(scores, dims, global_save_dir, model_name, quantization_techniques, inference_layers, task_category, plot_figures):\n","    plt.figure(figsize=(10, 5))\n","\n","    color_palette = sns.color_palette(\"husl\", len(inference_layers))\n","    layer_color_map = {layer: color for layer, color in zip(inference_layers, color_palette)}\n","\n","    reference_scores = {task: {layer: None for layer in inference_layers} for task in scores.keys()}\n","\n","    for task, task_scores in scores.items():\n","        for layer in inference_layers:\n","            reference_scores[task][layer] = task_scores['float32'][layer][0] if 'float32' in task_scores else None\n","\n","    for technique in quantization_techniques:\n","        for layer in inference_layers:\n","            all_relative_scores = []\n","            for task, task_scores in scores.items():\n","                if len(task_scores[technique][layer]) == len(dims):\n","                    relative_scores = []\n","                    for idx, score in enumerate(task_scores[technique][layer]):\n","                        if reference_scores[task][layer] is not None and reference_scores[task][layer] != 0:\n","                            relative_scores.append(score / reference_scores[task][layer])\n","                        else:\n","                            relative_scores.append(score)  \n","\n","                    all_relative_scores.append(relative_scores)\n","\n","            if not all_relative_scores:\n","                continue\n","\n","            average_relative_scores = np.mean(all_relative_scores, axis=0)\n","            plt.plot(dims, average_relative_scores, linestyle=LINE_STYLES.get(technique, '-'), color=layer_color_map[layer], linewidth=2, label=f\"{technique} (Layer {layer})\")\n","\n","    plt.xscale('log', base=2)\n","    plt.xticks(dims, [str(dim) for dim in dims])\n","    plt.xlabel('Dimension (base 10)')\n","    plt.ylabel('Relative Score')\n","    plt.ylim(-0.15, 1.05)\n","\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), borderaxespad=0)\n","\n","    if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}/all_averages_relative.png\", bbox_inches='tight')\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","def plot_all_technique_vs_layers(scores, dims, global_save_dir, model_name, quantization_techniques, inference_layers, task_category, plot_figures):\n","    plt.figure(figsize=(10, 5))\n","\n","    color_palette = sns.color_palette(\"husl\", len(quantization_techniques))\n","    technique_color_map = {technique: color for technique, color in zip(quantization_techniques, color_palette)}\n","    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', '+']\n","\n","    for dim_idx, dim in enumerate(dims):\n","        for technique in quantization_techniques:\n","            average_scores = []\n","            for layer in inference_layers:\n","                all_scores = [task_scores[technique][layer][dim_idx] for task_scores in scores.values() if len(task_scores[technique][layer]) == len(dims)]\n","\n","                if not all_scores:\n","                    continue\n","\n","                average_scores.append(np.mean(all_scores))\n","\n","            plt.plot(inference_layers, average_scores, linestyle=LINE_STYLES.get(technique, '-'), color=technique_color_map[technique],\n","                     marker=markers[dim_idx % len(markers)], linewidth=2, label=f\"{technique} ({dim}d)\")\n","\n","    plt.xticks(inference_layers, [str(layer) for layer in inference_layers])\n","    plt.xlabel('Inference Layer')\n","    plt.ylabel('Score')\n","    plt.ylim(-0.15, 1.05)\n","\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    plt.legend(loc='best')\n","\n","    if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}/all_vs_layers.png\")\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","\n","def plot_all_technique_vs_layers_relative(scores, dims, global_save_dir, model_name, quantization_techniques, inference_layers, task_category, plot_figures):\n","    plt.figure(figsize=(10, 5))\n","\n","    color_palette = sns.color_palette(\"husl\", len(quantization_techniques))\n","    technique_color_map = {technique: color for technique, color in zip(quantization_techniques, color_palette)}\n","    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', '+']\n","\n","    max_dim = max(dims)\n","    baseline_layer = inference_layers[0] \n","    baseline_scores = {}\n","\n","    for task_name, task_scores in scores.items():\n","        if \"float32\" in task_scores and baseline_layer in task_scores[\"float32\"]:\n","            if len(task_scores[\"float32\"][baseline_layer]) == len(dims):\n","                baseline_scores[task_name] = task_scores[\"float32\"][baseline_layer][dims.index(max_dim)]\n","\n","    if not baseline_scores:\n","        raise ValueError(\"Baseline scores could not be determined. Please check the inputs and data structure.\")\n","\n","    for dim_idx, dim in enumerate(dims):\n","        for technique in quantization_techniques:\n","            relative_scores = []\n","            for layer in inference_layers:\n","                all_scores = []\n","                for task_name, task_scores in scores.items():\n","                    if technique in task_scores and layer in task_scores[technique]:\n","                        if len(task_scores[technique][layer]) == len(dims):\n","                            score = task_scores[technique][layer][dim_idx]\n","                            baseline_score = baseline_scores.get(task_name, None)\n","                            if baseline_score is not None:\n","                                relative_score = score / baseline_score\n","                                all_scores.append(relative_score)\n","\n","                if all_scores:\n","                    average_relative_score = np.mean(all_scores)\n","                    relative_scores.append(average_relative_score)\n","\n","            plt.plot(inference_layers, relative_scores, linestyle=LINE_STYLES.get(technique, '-'), color=technique_color_map[technique],\n","                     marker=markers[dim_idx % len(markers)], linewidth=2, label=f\"{technique} ({dim}d)\")\n","\n","    plt.xticks(inference_layers, [str(layer) for layer in inference_layers])\n","    plt.xlabel('Inference Layer')\n","    plt.ylabel('Relative Score')\n","    plt.ylim(-0.15, 1.05)\n","\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","    \n","    plt.legend(loc='best')\n","\n","    if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModel/{model_name}/{task_category}/all_vs_layers_relative.png\")\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","\n","\n","\n","def generate_results(base_path, model_name, dims, quantization_techniques, tasks_classification, inference_layers, task_category, model_offset_path, plot_figures=True):\n","    scores = load_scores(base_path, model_name, dims, quantization_techniques, tasks_classification, inference_layers, model_offset_path)\n","\n","    if task_category != \"all\":\n","      for task in tasks_classification:\n","          plot_task_scores(scores, dims, GLOBAL_SAVE_DIR, model_name, task_category, task, plot_figures)\n","\n","    plot_technique_summary(scores, dims, GLOBAL_SAVE_DIR, model_name, quantization_techniques, inference_layers, task_category, plot_figures)\n","    plot_all_technique_averages(scores, dims, GLOBAL_SAVE_DIR, model_name, quantization_techniques, inference_layers, task_category, plot_figures)\n","    plot_all_technique_averages_relative(scores, dims, GLOBAL_SAVE_DIR, model_name, quantization_techniques, inference_layers, task_category, plot_figures)\n","\n","    scores_last_dim = load_scores(base_path, model_name, [1024], quantization_techniques, tasks_classification, inference_layers, model_offset_path)\n","    plot_all_technique_vs_layers(scores_last_dim, [1024], GLOBAL_SAVE_DIR, model_name, quantization_techniques, inference_layers, task_category, plot_figures)\n","    plot_all_technique_vs_layers_relative(scores_last_dim, [1024], GLOBAL_SAVE_DIR, model_name, quantization_techniques, inference_layers, task_category, plot_figures)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1Xb-pwkc-bKoP6BY63VElTT8Iyd8hUC2O"},"id":"RlYw69haBFjM","outputId":"86709b2c-6210-4e21-b20b-d1c79be30340"},"outputs":[],"source":["for task_category, tasks in tasks_dict.items():\n","    generate_results(MODEL_BASE_PATH, MODEL_NAME, dims, quantization_techniques, tasks, INFERENCE_LAYERS, task_category, MODEL_OFFSET_PATH, plot_figures=True)"]},{"cell_type":"markdown","metadata":{"id":"d5R6rWPeBFjM"},"source":["## Efficiency, Intramodels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"R3os3v_lBFjM"},"outputs":[],"source":["LINE_STYLES = {\n","    'float32': '-',\n","    'int8': '--',\n","    'binary': ':',\n","}\n","\n","def get_evaluation_time(base_path, model_name, embedding_size, quantization_method, benchmark_name, inference_layer, offset_path=\"no_model_name_available/no_revision_available\"):\n","    \"\"\"\n","    Function: Return the evaluation_time for one model, one embedding size, one quantization type, one benchmark, and one inference layer.\n","\n","    Args:\n","    base_path (str): Base directory containing the model folders.\n","    model_name (str): Name of the model.\n","    embedding_size (int): Embedding size of the model.\n","    quantization_method (str): Quantization method used.\n","    benchmark_name (str): Name of the benchmark.\n","    inference_layer (int): Inference layer number.\n","\n","    Returns:\n","    float: The evaluation_time for the specified configuration.\n","    \"\"\"\n","\n","    subfolder = f\"{model_name}_{embedding_size}_{quantization_method}_{inference_layer}\"\n","    file_path = os.path.join(base_path, subfolder, offset_path, f\"{benchmark_name}.json\")\n","\n","    try:\n","        with open(file_path, 'r') as file:\n","            data = json.load(file)\n","            evaluation_time = data[\"evaluation_time\"]\n","            return evaluation_time\n","    except (FileNotFoundError, KeyError) as e:\n","        print(f\"Error reading file {file_path}: {e}\")\n","        return None\n","\n","def load_evaluation_times(base_path, model_name, dims, quantization_techniques, tasks, inference_layers, offset_path):\n","    times = {task: {technique: {layer: [] for layer in inference_layers} for technique in quantization_techniques} for task in tasks}\n","    for task in tasks:\n","        for technique in quantization_techniques:\n","            for dim in dims:\n","                for layer in inference_layers:\n","                    time = get_evaluation_time(base_path, model_name, dim, technique, task, layer, offset_path=offset_path)\n","                    if time is not None:\n","                        times[task][technique][layer].append(time)\n","    return times\n","\n","def plot_task_evaluation_times(times, dims, global_save_dir, model_name, task_category, task, plot_figures):\n","    if not is_valid_task_category(task_category):\n","        raise ValueError(\"Invalid task category. Received {}, but expected one of: classification, clustering, sts, pairclass, retrieval, rerank, summ\".format(task_category))\n","\n","    color_palette = sns.color_palette(\"husl\", len(times[task][list(times[task].keys())[0]].keys()))\n","    inference_layers = list(times[task][list(times[task].keys())[0]].keys())\n","    layer_color_map = {layer: color for layer, color in zip(inference_layers, color_palette)}\n","\n","    for technique, layers_times in times[task].items():\n","        for layer, technique_times in layers_times.items():\n","            if len(technique_times) == len(dims):\n","                plt.plot(dims, technique_times, linestyle=LINE_STYLES.get(technique, '-'), color=layer_color_map[layer], linewidth=2, label=f\"{technique} (Layer {layer})\")\n","            else:\n","                print(f\"Skipping plot for {technique} on task {task} at layer {layer} due to dimension mismatch\")\n","\n","    plt.xscale('log', base=2)\n","    plt.xticks(dims, [str(dim) for dim in dims])\n","    plt.xlabel('Dimension (base 10)')\n","    plt.ylabel('Evaluation Time (s)')\n","\n","    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), borderaxespad=0) \n","\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}/{task}.png\", bbox_inches='tight')\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","\n","\n","\n","def plot_all_technique_evaluation_time_averages(times, dims, global_save_dir, model_name, quantization_techniques, inference_layers, task_category, plot_figures):\n","    plt.figure(figsize=(10, 5))\n","\n","    color_palette = sns.color_palette(\"husl\", len(inference_layers))\n","    layer_color_map = {layer: color for layer, color in zip(inference_layers, color_palette)}\n","\n","    for technique in quantization_techniques:\n","        for layer in inference_layers:\n","            all_times = [task_times[technique][layer] for task_times in times.values() if len(task_times[technique][layer]) == len(dims)]\n","\n","            if not all_times:\n","                continue\n","\n","            average_times = np.mean(all_times, axis=0)\n","            plt.plot(dims, average_times, linestyle=LINE_STYLES.get(technique, '-'), color=layer_color_map[layer], linewidth=2, label=f\"{technique} (Layer {layer})\")\n","\n","    plt.xscale('log', base=2)\n","    plt.xticks(dims, [str(dim) for dim in dims])\n","    plt.xlabel('Dimension (base 10)')\n","    plt.ylabel('Evaluation Time (s)')\n","    plt.ylim(0, None)\n","\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), borderaxespad=0)\n","\n","    if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}/all_averages.png\", bbox_inches='tight')\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","def plot_technique_evaluation_time_summary(times, dims, global_save_dir, model_name, quantization_techniques, inference_layers, task_category, plot_figures):\n","    if not is_valid_task_category(task_category):\n","        raise ValueError(\"Invalid task category. Received {}, but expected one of: classification, clustering, sts, pairclass, retrieval, rerank, summ\".format(task_category))\n","\n","    color_palette = sns.color_palette(\"husl\", len(inference_layers))\n","    layer_color_map = {layer: color for layer, color in zip(inference_layers, color_palette)}\n","\n","    for technique in quantization_techniques:\n","        plt.figure(figsize=(10, 5))\n","\n","        for layer in inference_layers:\n","            all_times = [task_times[technique][layer] for task_times in times.values() if len(task_times[technique][layer]) == len(dims)]\n","\n","            if not all_times:\n","                continue\n","\n","            average_times = np.mean(all_times, axis=0)\n","            plt.plot(dims, average_times, linestyle=LINE_STYLES.get(technique, '-'), color=layer_color_map[layer], linewidth=2, label=f\"Layer {layer}\")\n","\n","        plt.xscale('log', base=2)\n","        plt.xticks(dims, [str(dim) for dim in dims])\n","        plt.xlabel('Dimension (base 10)')\n","        plt.ylabel('Evaluation Time (s)')\n","        plt.ylim(0, None)\n","\n","        plt.grid(True, which=\"both\", ls=\"--\")\n","\n","        plt.legend(loc='best')\n","\n","        if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}\"):\n","            os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}\")\n","\n","        plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}/technique_{technique}.png\")\n","        if plot_figures:\n","            plt.show()\n","        plt.clf()\n","\n","\n","def plot_all_technique_evaluation_time_averages_relative(times, dims, global_save_dir, model_name, quantization_techniques, inference_layers, task_category, plot_figures):\n","    plt.figure(figsize=(10, 5))\n","\n","    color_palette = sns.color_palette(\"husl\", len(inference_layers))\n","    layer_color_map = {layer: color for layer, color in zip(inference_layers, color_palette)}\n","\n","    reference_layer = 24\n","\n","    for technique in quantization_techniques:\n","        for layer in inference_layers:\n","            relative_times = []\n","            for task, task_times in times.items():\n","                if 'float32' in task_times and reference_layer in task_times['float32']:\n","                    if len(task_times['float32'][reference_layer]) > 0:\n","                        reference_time = task_times['float32'][reference_layer][0]  \n","                    else:\n","                        continue  \n","\n","                    if len(task_times[technique][layer]) == len(dims):\n","                        relative_task_times = [t / reference_time for t in task_times[technique][layer]]\n","                        relative_times.append(relative_task_times)\n","\n","            if not relative_times:\n","                continue\n","\n","            average_relative_times = np.mean(relative_times, axis=0)\n","            plt.plot(dims, average_relative_times, linestyle=LINE_STYLES.get(technique, '-'), color=layer_color_map[layer], linewidth=2, label=f\"{technique} (Layer {layer})\")\n","\n","    plt.xscale('log', base=2)\n","    plt.xticks(dims, [str(dim) for dim in dims])\n","    plt.xlabel('Dimension (base 10)')\n","    plt.ylabel('Relative Evaluation Time')\n","    plt.ylim(0, None)\n","\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), borderaxespad=0)\n","\n","    if not os.path.exists(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/PerBenchmarkAndModelEfficiency/{model_name}/{task_category}/all_averages_evaluation_time_relative.png\", bbox_inches='tight')\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","\n","\n","def generate_evaluation_time_results(base_path, model_name, dims, quantization_techniques, tasks, inference_layers, task_category, model_offset_path, plot_figures=True):\n","    times = load_evaluation_times(base_path, model_name, dims, quantization_techniques, tasks, inference_layers, model_offset_path)\n","\n","    if task_category != \"all\":\n","      for task in tasks:\n","          plot_task_evaluation_times(times, dims, GLOBAL_SAVE_DIR, model_name, task_category, task, plot_figures)\n","\n","    plot_technique_evaluation_time_summary(times, dims, GLOBAL_SAVE_DIR, model_name, quantization_techniques, inference_layers, task_category, plot_figures)\n","    plot_all_technique_evaluation_time_averages(times, dims, GLOBAL_SAVE_DIR, model_name, quantization_techniques, inference_layers, task_category, plot_figures)\n","    plot_all_technique_evaluation_time_averages_relative(times, dims, GLOBAL_SAVE_DIR, model_name, quantization_techniques, inference_layers, task_category, plot_figures)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"19sW2IDUN4hmgbZkkkFltt73-HHOUwjv8"},"id":"lDxTGXr4BFjM","outputId":"f681a078-0beb-45ae-b612-3ca682a59fcf"},"outputs":[],"source":["for task_category, tasks in tasks_dict.items():\n","    generate_evaluation_time_results(MODEL_BASE_PATH, MODEL_NAME, dims, quantization_techniques, tasks, INFERENCE_LAYERS, task_category, MODEL_OFFSET_PATH, plot_figures=True)"]},{"cell_type":"markdown","metadata":{"id":"PnyhjgGVBFjM"},"source":["## Accuracy Compute TradeOff"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FfFIDZ0gBFjN"},"outputs":[],"source":["LINE_STYLES = {\n","    'float32': '-',\n","    'int8': '--',\n","    'binary': ':',\n","}\n","\n","def get_main_score(base_path, model_name, embedding_size, quantization_method, benchmark_name, inference_layer, offset_path=\"no_model_name_available/no_revision_available\"):\n","    \"\"\"\n","    Function 1: Return the main_score for one model, one embedding size, one quantization type, one benchmark, and one inference layer.\n","\n","    Args:\n","    base_path (str): Base directory containing the model folders.\n","    model_name (str): Name of the model.\n","    embedding_size (int): Embedding size of the model.\n","    quantization_method (str): Quantization method used.\n","    benchmark_name (str): Name of the benchmark.\n","    inference_layer (int): Inference layer number.\n","\n","    Returns:\n","    float: The main_score for the specified configuration.\n","    \"\"\"\n","\n","    subfolder = f\"{model_name}_{embedding_size}_{quantization_method}_{inference_layer}\"\n","    file_path = os.path.join(base_path, subfolder, offset_path, f\"{benchmark_name}.json\")\n","\n","    try:\n","        with open(file_path, 'r') as file:\n","            data = json.load(file)\n","            main_score = data[\"scores\"][\"test\"][0][\"main_score\"]\n","            return main_score\n","    except (FileNotFoundError, KeyError) as e:\n","        print(f\"Error reading file {file_path}: {e}\")\n","        return None\n","\n","def load_scores(base_path, model_name, dims, quantization_techniques, tasks, inference_layers, offset_path):\n","    scores = {task: {technique: {layer: [] for layer in inference_layers} for technique in quantization_techniques} for task in tasks}\n","    for task in tasks:\n","        for technique in quantization_techniques:\n","            for dim in dims:\n","                for layer in inference_layers:\n","                    score = get_main_score(base_path, model_name, dim, technique, task, layer, offset_path=offset_path)\n","                    if score is not None:\n","                        scores[task][technique][layer].append(score)\n","    return scores\n","\n","def calculate_memory_used(embedding_size, quantization_method):\n","    if quantization_method == 'float32':\n","        return embedding_size * 32\n","    elif quantization_method == 'int8':\n","        return embedding_size * 8\n","    elif quantization_method == 'binary':\n","        return embedding_size * 1\n","    else:\n","        raise ValueError(f\"Unknown quantization method: {quantization_method}\")\n","\n","def plot_accuracy_compute_tradeoff(scores, dims, quantization_techniques, global_save_dir, model_name, task_category, task, inference_layers, plot_figures):\n","    plt.figure(figsize=(10, 5))\n","    colors = sns.color_palette(\"husl\", len(quantization_techniques))\n","    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', '+'] \n","\n","    for technique_idx, technique in enumerate(quantization_techniques):\n","        for layer in inference_layers:\n","            technique_scores = scores[task][technique][layer]\n","            if len(technique_scores) == len(dims):\n","                memory_used = [calculate_memory_used(dim, technique) for dim in dims]\n","                for idx, (mem, score) in enumerate(zip(memory_used, technique_scores)):\n","                    plt.scatter(mem, score, label=f\"{technique} (Layer {layer}, {dims[idx]}d)\" if idx == 0 else \"\",\n","                                color=colors[technique_idx], marker=markers[idx % len(markers)], s=100, edgecolor='k')\n","\n","    plt.xlabel('Memory Used (bits)')\n","    plt.ylabel('Performance Score')\n","    plt.xscale('log', base=2)\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    handles, labels = plt.gca().get_legend_handles_labels()\n","    by_label = dict(zip(labels, handles))\n","    legend1 = plt.legend(by_label.values(), by_label.keys(), loc='best')\n","    plt.gca().add_artist(legend1)\n","\n","    marker_handles = [plt.Line2D([0], [0], marker=marker, color='w', markerfacecolor='k', markersize=10, linestyle='None') for marker in markers[:len(dims)]]\n","    marker_labels = [f'{dim}d' for dim in dims]\n","    legend2 = plt.legend(marker_handles, marker_labels, loc='lower right', bbox_to_anchor=(1, 0))\n","\n","    if not os.path.exists(f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}/{task}.png\", bbox_inches='tight')\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","def plot_average_accuracy_compute_tradeoff(scores, dims, quantization_techniques, global_save_dir, model_name, task_category, inference_layers, plot_figures):\n","    plt.figure(figsize=(12, 6))\n","    base_colors = sns.color_palette(\"husl\", len(quantization_techniques))\n","    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', '+'] \n","\n","    average_scores = {technique: {layer: [] for layer in inference_layers} for technique in quantization_techniques}\n","    for technique in quantization_techniques:\n","        for layer in inference_layers:\n","            all_scores = []\n","            for task, task_scores in scores.items():\n","                if len(task_scores[technique][layer]) == len(dims):\n","                    all_scores.append(task_scores[technique][layer])\n","\n","            if all_scores:\n","                average_scores[technique][layer] = np.mean(all_scores, axis=0)\n","\n","    for technique_idx, (technique, layers_scores) in enumerate(average_scores.items()):\n","        base_color = base_colors[technique_idx]\n","        for layer_idx, (layer, scores) in enumerate(layers_scores.items()):\n","            if len(scores) > 0:\n","                memory_used = [calculate_memory_used(dim, technique) for dim in dims]\n","                color = sns.set_hls_values(base_color, l=0.4 + 0.6 * (layer_idx / len(inference_layers)))\n","                for idx, (mem, score) in enumerate(zip(memory_used, scores)):\n","                    plt.scatter(mem, score, label=f\"{technique} (Layer {layer})\" if idx == 0 else \"\",  \n","                                color=color, marker=markers[idx % len(markers)], s=100, edgecolor='k')\n","\n","    plt.xlabel('Memory Used (bits)')\n","    plt.ylabel('Average Performance Score')\n","    plt.xscale('log', base=2)\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    handles, labels = plt.gca().get_legend_handles_labels()\n","    by_label = dict(zip(labels, handles))\n","    legend1 = plt.legend(by_label.values(), by_label.keys(), loc='upper left') \n","    plt.gca().add_artist(legend1)\n","\n","    marker_handles = [plt.Line2D([0], [0], marker=marker, color='w', markerfacecolor='k', markersize=10, linestyle='None') for marker in markers[:len(dims)]]\n","    marker_labels = [f'{dim}d' for dim in dims]\n","    legend2 = plt.legend(marker_handles, marker_labels, loc='lower right', bbox_to_anchor=(1, 0))\n","\n","    if not os.path.exists(f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}/all_averages.png\", bbox_inches='tight')\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","def plot_average_accuracy_compute_tradeoff_relative(scores, dims, quantization_techniques, global_save_dir, model_name, task_category, inference_layers, plot_figures):\n","    plt.figure(figsize=(12, 6))\n","    base_colors = sns.color_palette(\"husl\", len(quantization_techniques))\n","    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', '+']  \n","\n","    reference_scores = {task: {layer: None for layer in inference_layers} for task in scores.keys()}\n","\n","    for task, task_scores in scores.items():\n","        for layer in inference_layers:\n","            reference_scores[task][layer] = task_scores['float32'][layer][0] if 'float32' in task_scores else None\n","\n","    average_scores = {technique: {layer: [] for layer in inference_layers} for technique in quantization_techniques}\n","    for technique in quantization_techniques:\n","        for layer in inference_layers:\n","            all_relative_scores = []\n","            for task, task_scores in scores.items():\n","                if len(task_scores[technique][layer]) == len(dims):\n","                    relative_scores = []\n","                    for idx, score in enumerate(task_scores[technique][layer]):\n","                        if reference_scores[task][layer] is not None and reference_scores[task][layer] != 0:\n","                            relative_scores.append(score / reference_scores[task][layer])\n","                        else:\n","                            relative_scores.append(score)  \n","\n","                    all_relative_scores.append(relative_scores)\n","\n","            if all_relative_scores:\n","                average_scores[technique][layer] = np.mean(all_relative_scores, axis=0)\n","\n","    for technique_idx, (technique, layers_scores) in enumerate(average_scores.items()):\n","        base_color = base_colors[technique_idx]\n","        for layer_idx, (layer, scores) in enumerate(layers_scores.items()):\n","            if len(scores) > 0:\n","                memory_used = [calculate_memory_used(dim, technique) for dim in dims]\n","                color = sns.set_hls_values(base_color, l=0.4 + 0.6 * (layer_idx / len(inference_layers)))\n","                for idx, (mem, score) in enumerate(zip(memory_used, scores)):\n","                    plt.scatter(mem, score, label=f\"{technique} (Layer {layer})\" if idx == 0 else \"\",\n","                                color=color, marker=markers[idx % len(markers)], s=100, edgecolor='k')\n","\n","    plt.xlabel('Memory Used (bits)')\n","    plt.ylabel('Relative Average Performance Score')\n","    plt.xscale('log', base=2)\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    handles, labels = plt.gca().get_legend_handles_labels()\n","    by_label = dict(zip(labels, handles))\n","    legend1 = plt.legend(by_label.values(), by_label.keys(), loc='upper left') \n","    plt.gca().add_artist(legend1)\n","\n","    marker_handles = [plt.Line2D([0], [0], marker=marker, color='w', markerfacecolor='k', markersize=10, linestyle='None') for marker in markers[:len(dims)]]\n","    marker_labels = [f'{dim}d' for dim in dims]\n","    legend2 = plt.legend(marker_handles, marker_labels, loc='lower right', bbox_to_anchor=(1, 0))\n","\n","    if not os.path.exists(f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/AccuracyCompute/{model_name}/{task_category}/all_averages_relative.png\", bbox_inches='tight')\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","\n","def generate_accuracy_compute_results(base_path, model_name, dims, quantization_techniques, tasks, inference_layers, task_category, model_offset_path, plot_figures=True):\n","    scores = load_scores(base_path, model_name, dims, quantization_techniques, tasks, inference_layers, model_offset_path)\n","\n","    if task_category != \"all\":\n","        for task in tasks:\n","            pass\n","    plot_average_accuracy_compute_tradeoff(scores, dims, quantization_techniques, GLOBAL_SAVE_DIR, model_name, task_category, inference_layers, plot_figures)\n","    plot_average_accuracy_compute_tradeoff_relative(scores, dims, quantization_techniques, GLOBAL_SAVE_DIR, model_name, task_category, inference_layers, plot_figures)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"J0zKK7ZnBFjN"},"outputs":[],"source":["for task_category, tasks in tasks_dict.items():\n","  generate_accuracy_compute_results(MODEL_BASE_PATH, MODEL_NAME, dims, quantization_techniques, tasks, INFERENCE_LAYERS, task_category, MODEL_OFFSET_PATH, plot_figures=True)"]},{"cell_type":"markdown","metadata":{"id":"LdWUdvQLkcTS"},"source":["## Evaluation time vs. Accuracy TradeOff"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WvXkLSx3ke7H"},"outputs":[],"source":["def load_evaluation_times(base_path, model_name, dims, quantization_techniques, tasks, inference_layers, offset_path):\n","    times = {task: {technique: {layer: [] for layer in inference_layers} for technique in quantization_techniques} for task in tasks}\n","    for task in tasks:\n","        for technique in quantization_techniques:\n","            for dim in dims:\n","                for layer in inference_layers:\n","                    subfolder = f\"{model_name}_{dim}_{technique}_{layer}\"\n","                    file_path = os.path.join(base_path, subfolder, offset_path, f\"{task}.json\")\n","                    try:\n","                        with open(file_path, 'r') as file:\n","                            data = json.load(file)\n","                            evaluation_time = data[\"evaluation_time\"]\n","                            times[task][technique][layer].append(evaluation_time)\n","                    except (FileNotFoundError, KeyError) as e:\n","                        print(f\"Error reading file {file_path}: {e}\")\n","    return times\n","\n","def load_scores(base_path, model_name, dims, quantization_techniques, tasks, inference_layers, offset_path):\n","    scores = {task: {technique: {layer: [] for layer in inference_layers} for technique in quantization_techniques} for task in tasks}\n","    for task in tasks:\n","        for technique in quantization_techniques:\n","            for dim in dims:\n","                for layer in inference_layers:\n","                    subfolder = f\"{model_name}_{dim}_{technique}_{layer}\"\n","                    file_path = os.path.join(base_path, subfolder, offset_path, f\"{task}.json\")\n","                    try:\n","                        with open(file_path, 'r') as file:\n","                            data = json.load(file)\n","                            main_score = data[\"scores\"][\"test\"][0][\"main_score\"]\n","                            scores[task][technique][layer].append(main_score)\n","                    except (FileNotFoundError, KeyError) as e:\n","                        print(f\"Error reading file {file_path}: {e}\")\n","    return scores\n","\n","def plot_relative_evaluation_time_vs_scores(times, scores, dims, quantization_techniques, global_save_dir, model_name, task_category, inference_layers, plot_figures):\n","    plt.figure(figsize=(14, 8))\n","    base_colors = sns.color_palette(\"husl\", len(quantization_techniques))\n","    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', '+']\n","\n","    baseline_times = {}\n","    baseline_scores = {}\n","    for task in scores.keys():\n","        baseline_times[task] = times[task]['float32'][max(inference_layers)][dims.index(max(dims))]\n","        baseline_scores[task] = scores[task]['float32'][max(inference_layers)][dims.index(max(dims))]\n","\n","    for technique_idx, technique in enumerate(quantization_techniques):\n","        base_color = base_colors[technique_idx]\n","        for layer_idx, layer in enumerate(inference_layers):\n","            for task in scores.keys():\n","                task_scores = scores[task][technique][layer]\n","                evaluation_times = times[task][technique][layer]\n","                if len(task_scores) == len(dims) and len(evaluation_times) == len(dims):\n","                    relative_times = [time / baseline_times[task] for time in evaluation_times]\n","                    relative_scores = [score / baseline_scores[task] for score in task_scores]\n","                    color = sns.set_hls_values(base_color, l=0.4 + 0.6 * (layer_idx / len(inference_layers)))\n","                    for idx, (rel_time, rel_score) in enumerate(zip(relative_times, relative_scores)):\n","                        plt.scatter(rel_time, rel_score, label=f\"{technique} (Layer {layer})\" if idx == 0 else \"\",\n","                                    color=color, marker=markers[idx % len(markers)], s=100, edgecolor='k')\n","\n","    plt.xlabel('Relative Evaluation Time')\n","    plt.ylabel('Relative Task Score')\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    handles, labels = plt.gca().get_legend_handles_labels()\n","    by_label = {label.split(',')[0]: handle for handle, label in zip(handles, labels)}\n","    legend1 = plt.legend(by_label.values(), by_label.keys(), loc='upper left', bbox_to_anchor=(1, 1), title=\"Techniques and Layers\")\n","    plt.gca().add_artist(legend1)\n","\n","    marker_handles = [plt.Line2D([0], [0], marker=marker, color='w', markerfacecolor='k', markersize=10, linestyle='None') for marker in markers[:len(dims)]]\n","    marker_labels = [f'{dim}d' for dim in dims]\n","    legend2 = plt.legend(marker_handles, marker_labels, loc='upper left', bbox_to_anchor=(1, 0.5), title=\"Dimensions\")\n","\n","    plt.tight_layout(rect=[0, 0, 0.75, 1])  \n","\n","    if not os.path.exists(f\"{global_save_dir}/EvaluationTimeVsScores/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/EvaluationTimeVsScores/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/EvaluationTimeVsScores/{model_name}/{task_category}/evaluation_time_vs_scores.png\", bbox_inches='tight')\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n","\n","def plot_average_relative_evaluation_time_vs_scores(times, scores, dims, quantization_techniques, global_save_dir, model_name, task_category, inference_layers, plot_figures):\n","    plt.figure(figsize=(14, 8))\n","    base_colors = sns.color_palette(\"husl\", len(quantization_techniques))\n","    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', '+']\n","\n","    baseline_times = {}\n","    baseline_scores = {}\n","    for task in scores.keys():\n","        baseline_times[task] = times[task]['float32'][max(inference_layers)][dims.index(max(dims))]\n","        baseline_scores[task] = scores[task]['float32'][max(inference_layers)][dims.index(max(dims))]\n","\n","    average_relative_times = {technique: {layer: [] for layer in inference_layers} for technique in quantization_techniques}\n","    average_relative_scores = {technique: {layer: [] for layer in inference_layers} for technique in quantization_techniques}\n","\n","    for technique in quantization_techniques:\n","        for layer in inference_layers:\n","            for dim in dims:\n","                rel_times = []\n","                rel_scores = []\n","                for task in scores.keys():\n","                    if len(scores[task][technique][layer]) == len(dims) and len(times[task][technique][layer]) == len(dims):\n","                        rel_times.append(times[task][technique][layer][dims.index(dim)] / baseline_times[task])\n","                        rel_scores.append(scores[task][technique][layer][dims.index(dim)] / baseline_scores[task])\n","                if rel_times and rel_scores:\n","                    average_relative_times[technique][layer].append(np.mean(rel_times))\n","                    average_relative_scores[technique][layer].append(np.mean(rel_scores))\n","\n","    for technique_idx, technique in enumerate(quantization_techniques):\n","        base_color = base_colors[technique_idx]\n","        for layer_idx, layer in enumerate(inference_layers):\n","            color = sns.set_hls_values(base_color, l=0.4 + 0.6 * (layer_idx / len(inference_layers)))\n","            for idx, (rel_time, rel_score) in enumerate(zip(average_relative_times[technique][layer], average_relative_scores[technique][layer])):\n","                plt.scatter(rel_time, rel_score, label=f\"{technique} (Layer {layer}), {dims[idx]}d)\" if idx == 0 else \"\",\n","                            color=color, marker=markers[idx % len(markers)], s=100, edgecolor='k')\n","\n","    plt.xlabel('Relative Evaluation Time')\n","    plt.ylabel('Relative Task Score')\n","    plt.grid(True, which=\"both\", ls=\"--\")\n","\n","    handles, labels = plt.gca().get_legend_handles_labels()\n","    by_label = {label.split(',')[0]: handle for handle, label in zip(handles, labels)}\n","    legend1 = plt.legend(by_label.values(), by_label.keys(), loc='upper left', bbox_to_anchor=(1, 1), title=\"Techniques and Layers\")\n","    plt.gca().add_artist(legend1)\n","\n","    marker_handles = [plt.Line2D([0], [0], marker=marker, color='w', markerfacecolor='k', markersize=10, linestyle='None') for marker in markers[:len(dims)]]\n","    marker_labels = [f'{dim}d' for dim in dims]\n","    legend2 = plt.legend(marker_handles, marker_labels, loc='upper left', bbox_to_anchor=(1, 0.5), title=\"Dimensions\")\n","\n","    plt.tight_layout(rect=[0, 0, 0.75, 1]) \n","\n","    if not os.path.exists(f\"{global_save_dir}/EvaluationTimeVsScores/{model_name}/{task_category}\"):\n","        os.makedirs(f\"{global_save_dir}/EvaluationTimeVsScores/{model_name}/{task_category}\")\n","\n","    plt.savefig(f\"{global_save_dir}/EvaluationTimeVsScores/{model_name}/{task_category}/average_evaluation_time_vs_scores.png\", bbox_inches='tight')\n","    if plot_figures:\n","        plt.show()\n","    plt.clf()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WOTvRrbAkpNW"},"outputs":[],"source":["for task_category, tasks in tasks_dict.items():\n","  times = load_evaluation_times(MODEL_BASE_PATH, MODEL_NAME, dims, quantization_techniques, tasks, INFERENCE_LAYERS, MODEL_OFFSET_PATH)\n","  scores = load_scores(MODEL_BASE_PATH, MODEL_NAME, dims, quantization_techniques, tasks, INFERENCE_LAYERS, MODEL_OFFSET_PATH)\n","\n","  plot_average_relative_evaluation_time_vs_scores(times, scores, dims, quantization_techniques, GLOBAL_SAVE_DIR, MODEL_NAME, task_category, INFERENCE_LAYERS, True)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMTBUz6RGvxhUI2+EfWtFxF","name":"","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
